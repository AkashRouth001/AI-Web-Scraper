
# ğŸ•·ï¸ AI-Powered Web Scraper with Local LLaMA

This project is an **AI-enhanced web scraper** built with Python that can extract structured data from any website based on natural language prompts. It combines **Selenium** for scraping, **BeautifulSoup** for cleaning HTML, and **LangChain + Ollama** to run **open-source LLaMA models locally**, giving you powerful data extraction capabilities without using paid APIs like OpenAI.

---

## ğŸš€ Features

- ğŸŒ Scrape any website (even dynamic ones rendered with JavaScript)
- ğŸ›¡ï¸ Bypass captchas and IP bans using Bright Data's remote browser
- ğŸ§¹ Clean and filter the HTML using BeautifulSoup
- ğŸ§  Use local LLaMA models (via Ollama) to extract specific data based on user prompts
- ğŸ“„ Chunk content automatically to fit within LLM token limits
- ğŸ–¥ï¸ Streamlit-based user interface

---

## ğŸ“¸ Demo

<img src="https://github.com/your-username/AI-Web-Scraper/assets/demo-screenshot.png" width="600"/>

---

## ğŸ”§ How It Works

1. **User enters a URL** in the Streamlit interface.
2. The app uses **Selenium** with **Bright Data's scraping browser** to load and capture the website.
3. The HTML is cleaned using **BeautifulSoup** to remove scripts, styles, and noise.
4. Cleaned content is split into manageable chunks.
5. Each chunk is passed to a **locally running LLaMA model** using **LangChain** and **Ollama**.
6. The AI parses the content based on the userâ€™s prompt and returns the result.

---

## ğŸ§ª Example Prompts

- `List all product names and prices.`
- `Summarize the content of this page.`
- `Extract all links to property listings.`

---

## ğŸ“¦ Installation

### 1. Clone the repo

```bash
git clone https://github.com/your-username/AI-Web-Scraper.git
cd AI-Web-Scraper


### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up `.env` file

Create a `.env` file in the root directory and add your Bright Data WebDriver URL:

```env
SBR_WEBDRIVER=https://brd.superproxy.io:9222/session/YOUR_SESSION_ID
```

### 5. Install and run Ollama (LLaMA)

Download and install [Ollama](https://ollama.com/) on your system and run the LLaMA model locally:

```bash
ollama run llama3
```

---

## â–¶ï¸ Running the App

Once everything is set up, launch the Streamlit app:

```bash
streamlit run main.py
```

---

## ğŸ“ Project Structure

```
AI-Web-Scraper/
â”œâ”€â”€ main.py             # Streamlit UI
â”œâ”€â”€ scrape.py           # Web scraping logic
â”œâ”€â”€ parse.py            # AI parsing logic with LangChain & Ollama
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ .env                # Environment variables (not uploaded to GitHub)
â”œâ”€â”€ chromedriver        # Optional local Selenium driver
```

---

## ğŸ“Œ Requirements

* Python 3.8+
* Chrome or Chromium
* [Ollama](https://ollama.com/)
* Bright Data scraping browser session (free trial available)

---

## ğŸ§  Future Improvements

* Async processing for faster chunk parsing
* Add OpenAI/GPT support as fallback
* Support for paginated or multi-page scraping
* Deployable version on HuggingFace or Streamlit Cloud

---

## ğŸ™‹â€â™‚ï¸ Author

**Akash Routh**
Passionate about AI, automation, and making the web smarter.
ğŸ”— [GitHub](https://github.com/AkashRouth001)

---

## ğŸ›¡ï¸ License

This project is licensed under the MIT License.

```

---

Let me know if you'd like:
- A version with advanced features documented
- Markdown badges (build, stars, license, etc.)
- A `demo.mp4` or GIF if you want to include a quick visual demo

I can also help create a short **demo video** script or thumbnail if you're planning to showcase this online.
```
